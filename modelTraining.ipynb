{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the dataset\n",
    "image_dir = 'dataset\\ham10000'  # Replace with your dataset images path\n",
    "metadata_path = 'dataset\\ham10000_metadata.csv'  # Replace with your metadata CSV file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata\n",
    "metadata = pd.read_csv(metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the diagnosis labels into integers\n",
    "le = LabelEncoder()\n",
    "metadata['label'] = le.fit_transform(metadata['diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes (unique diagnoses)\n",
    "num_classes = len(metadata['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load and preprocess each image\n",
    "def load_and_preprocess_image(img_id, label, img_size=(128, 128)):\n",
    "    img_id_str = img_id.numpy().decode('utf-8')\n",
    "    img_path = os.path.join(image_dir, img_id_str + '.jpg')\n",
    "    image = load_img(img_path, target_size=img_size)\n",
    "    image = img_to_array(image)\n",
    "    image = image / 255.0  # Normalize to [0,1]\n",
    "    return image, label\n",
    "\n",
    "# Adjust the function to handle TensorFlow tensors properly\n",
    "def load_and_preprocess_image_tf(img_id, label):\n",
    "    image, label = tf.py_function(\n",
    "        func=load_and_preprocess_image, \n",
    "        inp=[img_id, label], \n",
    "        Tout=[tf.float32, tf.int32]  # Ensure the label is returned as int32\n",
    "    )\n",
    "    image.set_shape((128, 128, 3))\n",
    "    label.set_shape([])\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorFlow Dataset from the metadata\n",
    "image_labels_ds = tf.data.Dataset.from_tensor_slices((\n",
    "    metadata['isic_id'].values,\n",
    "    metadata['label'].values.astype(np.int32)  # Cast labels to int32\n",
    "))\n",
    "\n",
    "# Map the dataset with the correct function\n",
    "dataset = image_labels_ds.map(load_and_preprocess_image_tf, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation using tf.keras.layers\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.3),\n",
    "    tf.keras.layers.RandomZoom(0.3),\n",
    "    tf.keras.layers.RandomContrast(0.2),\n",
    "    tf.keras.layers.RandomBrightness(0.2),\n",
    "    tf.keras.layers.RandomTranslation(0.2, 0.2),\n",
    "])\n",
    "\n",
    "# Apply data augmentation to the training dataset\n",
    "augmented_train_dataset = dataset.map(\n",
    "    lambda x, y: (data_augmentation(x, training=True), y),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch and prefetch the dataset for training\n",
    "batch_size = 32\n",
    "train_size = int(0.8 * len(metadata))\n",
    "val_size = int(0.1 * len(metadata))\n",
    "\n",
    "train_dataset = augmented_train_dataset.take(train_size).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = dataset.skip(train_size).take(val_size).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = dataset.skip(train_size + val_size).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CNN model using Keras layers with MobileNetV2 Backbone\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras import layers, models\n",
    "from keras_tuner import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all labels from the training dataset\n",
    "all_labels = []\n",
    "\n",
    "for _, y_batch in train_dataset:\n",
    "    all_labels.extend(y_batch.numpy())\n",
    "\n",
    "# Calculate the class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(all_labels),\n",
    "    y=all_labels\n",
    ")\n",
    "\n",
    "# Create a dictionary mapping class indices to weights\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "96                |96                |conv_1_filter\n",
      "5                 |5                 |conv_1_kernel\n",
      "416               |416               |dense_1_units\n",
      "\n",
      "WARNING:tensorflow:From d:\\fajar\\Projects\\Skincancer\\.conda\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "def build_model(hp):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(128,128,3)),\n",
    "        layers.Conv2D(filters=hp.Int('conv_1_filter',min_value=32,max_value=256, step=32),\n",
    "                      kernel_size=hp.Choice('conv_1_kernel',values=[3, 5]),\n",
    "                      activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(units=hp.Int('dense_1_units', min_value=32, max_value=512, step=32),\n",
    "                     activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    directory='my_dir',\n",
    "    project_name='skincancer_hyperparam_tuning'\n",
    ")\n",
    "\n",
    "tuner.search(train_dataset, validation_data=val_dataset, epochs=10)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters()[0]\n",
    "print(f\"Best hyperparameters: {best_hps.values}\")\n",
    "\n",
    "# Build and train the model with the best hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping, lr_scheduler],\n",
    "    class_weights=class_weight_dict,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"/content/drive/MyDrive/skin_cancer_cnn_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To predict and display the output probabilities\n",
    "sample_image = next(iter(test_dataset.take(1)))[0]\n",
    "predictions = model.predict(sample_image)\n",
    "predicted_class = np.argmax(predictions[0])\n",
    "predicted_label = le.inverse_transform([predicted_class])[0]\n",
    "confidence = np.max(predictions) * 100\n",
    "\n",
    "print(f\"Prediction: {confidence:.2f}% similar to {predicted_label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
